Building DAG of jobs...
Using shell: /usr/bin/bash
Provided cluster nodes: 100
Job counts:
	count	jobs
	2	AlleleAtlas
	1	MergeVCFs
	1	Subset
	1	all
	5
Select jobs to execute...

[Sat Feb 12 18:15:38 2022]
rule MergeVCFs:
    input: results/ImputeRP_RP_Chr04.vcf.gz, results/ImputeNRP_NRP_Chr04.vcf.gz
    output: results/MergeVCFs_RP_NRP_AllAccessions_Chr04.vcf.gz
    jobid: 2
    wildcards: chr=04

Submitted job 2 with external jobid 'Submitted batch job 24195029'.
[Sat Feb 12 18:25:49 2022]
Finished job 2.
1 of 5 steps (20%) done
Select jobs to execute...

[Sat Feb 12 18:25:49 2022]
rule Subset:
    input: results/MergeVCFs_RP_NRP_AllAccessions_Chr04.vcf.gz, resources/EliteUSOnly.txt
    output: results/MergeVCFs_RP_NRP_EliteUSOnly_Chr04.vcf.gz
    jobid: 10
    wildcards: chr=04

WorkflowError in line 1 of /storage/hpc/group/bilyeu/nad7wf/allele_atlas_pipeline/rules/custom/Subset.smk:
'Wildcards' object has no attribute 'job-name'
  File "/cluster/software/conda-envs/envs/snakemake-5.3.0/lib/python3.6/site-packages/snakemake/executors/__init__.py", line 135, in run_jobs
  File "/cluster/software/conda-envs/envs/snakemake-5.3.0/lib/python3.6/site-packages/snakemake/executors/__init__.py", line 966, in run
